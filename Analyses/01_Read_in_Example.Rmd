---
title: "01_Mars_Shape_Phylo_Prep"
author: "Vera Weisbecker"
date: "26 June 2019"
output: html_document
---


#Set working directory, load required packages

```{r }

#This line sets the working directory to be where this code is stored. At least on most computers. If this doesn't work, you should be able to just enter the whole file path wherever you need to read a file (or write to a file). In that case, beware of the slashes - R works with forward slashes (/) but PCs tend to work with backward slashes. There is a way of forcing slashes the right way but it's easier to just manually change.
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

## Loading the libraries (and installing if necessary)
#geomorph is the package we will use here. But there is at least one other, the Morpho package by Stephan Schlager which has some nifty additional functionalities. If you don't like working in a Brownian Motion context, you can also use Julien Clavel's mvMORPH package for some high-powered stats. 
if(!require(geomorph)) install.packages("geomorph")

#Rvcg loads 3D files much faster than the geomorph functions  read.ply. You can also use the package to convert between mesh formats (e.g. ply vs. stl) and to decimate meshes to a particular face number
if(!require (Rvcg)) install.packages("Rvcg")

```


# Load 3d coordinates from the files you exported from Checkpoint. This is for Morphologika files and the code comes from Emma Sherratt
 
```{r }

filelist <- list.files(path= "../Data/Raw/Coordinates", pattern = "*.txt")

#Next step is to remove the Museum IDs from the specimen names (in this particular case. Each dataset is different but each specimen needs a unique name and the specimen names need to match up with any classifier files you are using)  

#First, split your file lists by lower scores, so the first word in the file name becomes column 1 etc. This only works if your files each have the exact same naming convention, e.g. Genus_Species_MuseumNumber.

  tmp <- matrix(unlist(strsplit(filelist, "_")),ncol=3,byrow = TRUE)   

  filelist_species <- paste(tmp[,1], tmp[,2], sep="_")

#I am presenting the below as just a chunk of unexplained code to import Morphologica-exported files from Checkpoint. Other files have different ways of reading in but there should be ample documentation about this online.  
  names <- gsub (".txt", "", filelist) # extracts names of specimens from the file name
  filelist <- paste("../Data/Raw/Coordinates/", filelist, sep="") # rename with path
  coords <- NULL # make empty object that will be filled with 3D array of coordinate data
  
  #This loops takes each file, appends it to the 3D array, and gives it the species names we created above.
  for (i in 1:length(filelist)){
    temp  <- read.morphologika(filelist[i]) 
    k <- dim(temp)[1] 
    coords <- rbind(coords, two.d.array(temp)) }
  Data <- arrayspecs(coords, k, 3) 
  dimnames(Data)[[3]] <- filelist_species
  remove(i, filelist, names, k, coords, temp) # clean up environment
  
  #Check the data read in ok. $dim should have number of landmarks (58), number of dimensions (3), number of specimens (10), and the specimen names.
  
  attributes(Data)
  
  #To navigate to a particular specimen (e.g. the second one in the array, which you know from the attributes() command above is called "Aepyprymnus_rufescens")
  Data[,,7]
  
  
  #Double check that the 3D info is entered properly by checking landmark numbers
  plot3d(Data[,,7], asp=FALSE, col="blue")
  text3d(Data[,,7], texts=c(1:58))
  
```


# You can  read in a surface file if you want - this is useful for later visualising shape change and making sure the coords and the 3D files line up. You might want to do this later if you want to warp a mesh to the mean shape (NOT explained here, just saying)

```{r}

#This is a mesh I crunched down to 600,000 faces. It's 18 kb.
Apyg <- vcgPlyRead("../Data/Raw/Acrobates_pygmaeus_J7579.ply")

#the default mesh colour is black, so pick a pretty colour
shade3d(Apyg, col="hotpink")



```

#Read a classifier file that contains other information about the species, such as info on locomotion type, diet, and clade; this classifier file is used for analysing shape relative to whatever data are entered in that file for all species. The row names of the classifier should match the dimnames of your array. The easiest way to do this is to 1) do a great job at giving good file names to your coordinates and 2) use the filelist() command to give both the 3D coordinates and the classifier file identical rownames/dimnames. Try not to manually type the names into the classifier. 


```{r}

speclist<- read.delim("../Data/Raw/Trial_classifier_list.txt")
rownames(speclist) <-speclist$Specimen

#Double check that the order of your Array and the order of your specimen list are the same (they will need to talk to each other!). This has to return "TRUE" for all. 

dimnames(Data)[[3]]==rownames(speclist)

```


#If you have curve or patch semilandmarks, you need to make a separate file for them that allows them to be identified. I won't do it here but the best way to determine which curve landmarks are which number is to plot the landmarks: 

```{r}

#this opens a new 3D window so it doesn't plot over the previous  
open3d()

#a transparent 3d object. alpha is 0=transparent, 1=opaque. Alpha takes a lot of graphica memory, so only use if you really need to.
shade3d(Apyg, alpha = 0.4, col="hotpink")

# put on the landmarks as spheres, radius is how big. The default radius value can be really large or small relative to the specimen, so you might have to play a bit with this
spheres3d(Data[,,1],  col="green", radius = 0.1)

#text for landmark numbers, e.g. if you want to determine the lm numbers for your patches and curves
text3d(Data[,,1], texts=c(1:58), pos = 2, cex=1.2, font = 2)

```

# if you are using patch/curve semilandmarks, use the above to figure out what numbers they are. I normally enter them into csv files manually. I have example csv files in teh Data/Raw folder that do not match this dataset,just fyi. 

#many people also add a symmetry removal step to their analyses, and then you would also have to add a file on the pairs of landmarks that are opposite each other on the midline. I am split on the usefulness of this and it makes the code very complicated. Most of my projects have it but I am moving away from it though because it just over-processes the data.

You read these files in like:  

```{r}

#a table with landmark before the sliding landmark (can be sliding itself), the sliding landmark itself, and the landmark after the sliding landmark.
curveLM <- read.csv("../Data/Raw/EXAMPLE_curveslide_manual.csv")

#just a vector with which landmarks are to be treated as surface sliding.
surfaceLM <- read.csv("../Data/Raw/EXAMPLE_surfslide_manual.csv")


```


#GPA for all coordinates

```{r}
#Run GPA. If you have sliding and curve semilandmarks, you also add curves = curveLM, surfaces=surfaceLM
GPA <- gpagen(Data)

#This is what's in a GPA. Note the separate Centroid sizes - a good way of checking if they are all correct is to check if your smallest specimen also has the smallest C size. There is a lot of potential for error in specimen dimensions during checkpoint exports, so it's worth keeping a good eye on the Csizes at all times. 

attributes(GPA)

#you can navigate to each specimen like so:
GPA$coords[,,8]

#Find the specimen that is closest to the overall mean shape

findMeanSpec(GPA$coords)

#Or create a reference configuration that is the actual mathematical mean shape. You can also use GPA$consensus for this, it's the same.

Ref <- mshape(GPA$coords)


```

#run a basic PCA, which arranges the specimens according to where the greates amount of variation is (PC1), then the 2nd greatest (PC2)... etc. This is good for summarising but don't be tempted to analyse the PCAs much. There are better ways of asking if there is a difference between groups than checking a PCA plot (Weisbecker et al. 2019).

```{r}

PCA <- gm.prcomp(GPA$coords)

#This is what's in a PCA
attributes(PCA)

#How are specimesn arranged on a PC1 vs PC 2 plot?

palette <- rainbow(length(levels(as.factor(speclist$Diet))))


plot(PCA$x[,2]~ PCA$x[,1], pch=19, col= palette[as.factor(speclist$Diet)], cex=1)
text(  PCA$x[,2]~ PCA$x[,1], labels=rownames(speclist)  )

# How does PC1 relate to centroid size? for fiddling with image settings, go to R console and type in ?plot. Note I'm log-transforming csize because animals tend to evolve on an exponential scale when it comes to size. If we didn't log, we would have all the small specimens bunched on the left and the relationship would likely not be linear.

plot(PCA$x[,1]~ log(GPA$Csize), pch=19)
text(  PCA$x[,1]~ log(GPA$Csize), labels=rownames(speclist)  )

#This shows the differences between a hypothetical specimen's landmark configuration at the lower scores of PC1 - balls - and compares it to a hypothetical landmark configuration for the higher scores of PC1 (where the hairs point to)
plotRefToTarget(PCA$shapes$shapes.comp1$min, PCA$shapes$shapes.comp1$max, method="vector")


```

#If you want to run a linear model of shape relative to a predictor - this can be like any other linear model, with a continuous or discrete predictor and you can have a multi-predictor model with or without interaction terms. All just like with basic linear modelling. 
```{r}

#Allometry is one of the most common continuous variables that people would check, and it is much better to run this regression here rather than PCs. Again, logging Csize here

Allometry <- procD.lm(GPA$coords~ log(GPA$Csize))

#You can get a summary of this regression by typing
summary(Allometry)

#You can check the model fit as you usually would by plotting the model, this gives you several diagnostic plots you need to click through

plot(Allometry)

#You can also plot the actual regression, you have to specify that that is what you want and include the predictor again. If you type in reg.type="PredLine", you get a line of where each specimen is _projected_ to be for it's size. I don't find that very useful though because it masks the amount of variation that each specimen is off the common allometric line.
plot(Allometry, type="regression", predictor=log(GPA$Csize), reg.type = "RegScore")

#If you want to do a prettier job than the baseline plot here, you can extract the predictor and fitted scores by saving the entire plot
Plot_allometry <- plot(Allometry, type="regression", predictor=log(GPA$Csize), reg.type = "RegScore")

#Then you can plot using whatever function takes your fancy: 
plot(Plot_allometry$RegScore~Plot_allometry$plot_args$x,pch=19, col= palette[as.factor(speclist$Diet)], cex=3)


#You can also pull out the change of shape at the smallest and largest specimen size (so not the actual specimen, but the shape of the specimen that would be expected for its size)This is a bit fiddly but here is the code: 


preds <- shape.predictor(Allometry$GM$fitted, 
                         x=Plot_allometry$RegScore,
                         predmin= min(Plot_allometry$RegScore),
                          predmax=max(Plot_allometry$RegScore)
                         )

#then you can use the below to visualise the shape differences between smallest and largest. Hilarious!!
plotRefToTarget(preds$predmin, preds$predmax, method="vector", radius=0.008)


#You can also do the same thing with a factor or both, e.g.

LM <- procD.lm(GPA$coords~ GPA$Csize * speclist$Diet)

summary(LM)




```



#Package into RDA file. This is useful if you want to keep everything your computer just did, and re-open it for use in another file. For example, you might use this file to do the basic loading, GPA, PCA of data but then later do something more fancy or a specific analysis with everything already loaded. 
```{r}

save.image(file="../Data/Processed/Read_in_Data.rda")

#Now if you clear the environment
rm(list = ls())

#You can now get it all back by doing:

load (file="../Data/Processed/Read_in_Data.rda")


```

